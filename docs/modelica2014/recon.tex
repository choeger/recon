% sample file for Modelica Conference paper

\documentclass[11pt,a4paper,twocolumn]{article}
\usepackage{graphicx}
\graphicspath{{fig/}}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}    %% european characters can be used
\usepackage{lmodern,amsmath,mathptmx,url}      %% recommended for readable pdf
\pagestyle{empty}                %% no page numbers!
\usepackage{geometry}            %% please don't change geometry settings!
\geometry{left=20mm, right=20mm, top=25.4mm, bottom=25mm, noheadfoot, columnsep=8mm}
\parindent0pt
\bibliographystyle{ieeetr}

% some additional packages
\usepackage{listings} % for code listings
\usepackage{color}
\usepackage[hidelinks=true]{hyperref}

% usefull commands
\newcommand{\myr}{\textsuperscript{\textregistered}}
\newcommand{\ud}{\mathrm{d}}
\newcommand{\matx}[1]{\mathbf{#1}}
\newcommand{\recon}{\texttt{recon}}
\newcommand{\wall}{\texttt{wall}}
\newcommand{\meld}{\texttt{meld}}
\newcommand{\msgpack}{\texttt{msgpack}}
\newcommand{\code}[1]{\texttt{#1}} % make quoting code text a bit simpler

\begin{document}

\title{\textbf{{\small Modelica'2014}\\
    \recon -- Web and network friendly simulation data formats}}

\author{Michael Tiller\\
  \href{http://xogeny.com}{Xogeny Inc.}, USA\\
  \href{mailto:michael.tiller@xogeny.com}
       {\nolinkurl{michael.tiller@xogeny.com}}
  \and Peter Harman\\
  \href{http://www.cydesign.com}
       {CyDesign}, UK\\
  \href{mailto:peter@cydesign.com}{\nolinkurl{peter@cydesign.com}}}
\date{} % <--- leave date empty
\maketitle\thispagestyle{empty} %% <-- you need this for the first page

\section*{Abstract}

There are many different commonly used file formats for storing time
series data.  Most of these file formats are designed with the
assumption that the file itself will be locally available to the
software that will be reading or writing the data stored in them.
While this assumption is an excellent one for desktop based tools with
direct access to disk drives capable of moving virtually
instantaneously around from sector to sector, there are growing number
of applications for which local access is not necessarily available.
For these applications, we've initiated the \recon project to develop
more suitable formats.

With the emergence of web and cloud based modeling and simulation
technologies, the time has come to explore file formats specifically
optimized for non-desktop applications.  In this paper, we present a
new set of file formats that are specifically designed for web and
cloud based approaches.  This paper reviews the key requirements for
web and cloud enabled applications and then presents a specification
for two file formats that address those requirements.

When considering the various use cases that drove our requirements, we
recognized that two different file formats were actually required.
The first format, the \wall format, is optimized for writing.  The
other format, the \meld format, is optimized for reading over a
network.  We will discuss the importance of each of these formats and
describe the use cases for which they are most appropriate.

In the open tradition of the Mdoelica Association, the authors have
made specifications and implementations for these formats available as
open source libraries with the hope that they will benefit the
community as a whole.

\paragraph{Keywords:}\emph{modelica, FMI, simulation results,
  cloud, web, open source}

\section{Introduction}
\label{sec:intro}

Several groups have examined the issue of standardized file formats
\cite{AndreasHDF5,GallHDF5} in the context of Modelica.  In keeping
with the principals of the Modelica Association, an ideal choice would
be a production ready format that is open source and cross-platform.
With this requirements in mind, most people consider HDF5 a natural
choice.  There are already open source implementations and the file
format has been widely used.  In fact, it has even been adopted by The
MathWorks for use in MATLAB.

But HDF5 has some important drawbacks.  The first is that it is not
truly cross-platform.  The reference implementation of HDF5 is written
in C.  The implementation is primarily targeted for use with in C, C++
or Fortran applications.  While there are various libraries available
for reading HDF5 on the Java platform\cite{HDFJava}, they are
incomplete and awkward to use.

Another issue with HDF5 is that for simple time-series data it is over
engineered.  HDF5 is feature rich, of that there is no question, but
these features come at the cost of complexity.  This is why you see
very few implementations outside of the reference implementation.
Furthermore, the file format makes extensive use of ``seek''
operations and assumes they are relatively in expensive.  This
assumption is reasonable if you are able to communicate directly with
the hard drive that the files are stored on, but it isn't reasonable
when these files are only available through the network.

There are, of course, many standards for manipulating data in web or
cloud based environments.  The most popular formats, by far, are
Javascript Object Notation (or JSON, for short) and XML.  There are a
number of different approaches to marshalling and unmarshalling data
for consumption by cross-platform tools.  Approaches like Google
Protocol Buffers\cite{GPB}, Avro\cite{Avro}, Thrift\cite{Thrift} and
others are working hard to try and knit together the various ``big
data'' tools like Hadoop\cite{Hadoop}, Storm\cite{Storm} and ???.

So where does that leave us?  Should we adopt the tried and true
standards from the engineering world and simply live with their lack
of interoperability with important platforms like Java or Javascript
and poor performance when accessed remotely?  Or, should we try and
adapt tools from the ``big data'' world, that were developed for quite
different use cases, to work in the engineering world.

In some sense, we've chosen a compromise.  As we will see shortly, the
\wall and \meld formats are fundamentally derived from the
\msgpack\cite{msgpack} specification.  This gives us excellent cross
platform compatibility.  But \msgpack is simply a serialization
protocol.  To address some of our more important concerns, to be
discussed shortly, we needed to design a structural context in which
to employ \msgpack. So in this sense, we've created a set of original
file formats that leverage open standards but re-purpose them for
modeling and simulation applications.

\section{Goals}
\label{sec:background}

After independently reviewing various file formats, the authors were
not happy with the existing options for web and cloud based modeling
and simulation.  The \recon project started as a discussion about
requirements.  For our applications, the following requirements were
identified:

\subsection{#1 - Adding Data}

% 1) It should be easy to append data to a file.

Simulations are constantly producing additional data.  For this
reason, adding new data to an existing file is an operation performed
many times during a simulation.  For this reason, adding data to an
existing file should be fast and easy.  The key thing is to avoid having to
rewrite previous data or, even worse, move data around within the
file.  For this reason, the ideal solution is to have the ability to
simply append new data at the end of the file.

\subsection{#2 - Minimizing I/O}

% 2) It must be possible to extract individual signals with a minimum of I/O.

In web and cloud based applications, it is not always practice to
download the complete set of results for a simulation into the browser
environment.  There are many use cases where it would be best to be
able to access results ``on demand''.  In these environments, such
requests for data will be done via HTTP\cite{HTTP}.  However, each of
these requests will come with far greater latency than a simple
request to read from a disk drive and far less bandwidth.  As such, we
would want to minimize the number of such requests and the amount of
data necessary to transmit in each request.  This means we need a way
to ``cluster'' the data we are interested in so as to minimize both
the number of requests required and minimize the amount of data in
each response.

\subsection{#3 - Cross Platform Support}

% 3) The format must work across languages.

The file formats developed as part of the \recon project are targeted
at web and cloud based applications.  Client side web programming is
dominated by Javascript.  On the other hand, server side programming
is done in a wide variety of languages (*e.g.,* Java, Python,
Javascript).  Meanwhile, numerical analyses such as simulation are
typically done in languages like C, C++ and FORTRAN.  For this reason,
it should be possible to implement libraries in all of these languages
for generating and extracting simulation results.

\subsection{#4 - Aliasing}

% 4) It must be possible to define aliases.

One of the common patterns in component-oriented modeling approaches
like Modelica is that many variables end up with exactly the same
solution trajectories.  When storing simulation results from such
tools, it is useful to recognize that considerable disk space can be
saved by recognizing the fact that these variables all share a common
underlying solution trajectory.  Typically, the data for each unique
solution trajectory is stored once and each variable is simply a
reference to the underlying solution trajectory.  Even more storage
can be saved by recognizing that some trajectories are related to
other trajectories by very simple transformations (*.e.g.,* a simple
sign change).

For this reason, it is very useful if these kinds of relationships are
directly represented in the file format itself.

\subsection{#5 - Data Types}

% 5) It should be possible to store "structures/dictionaries" as well
% as regular array types.

When dealing with simulation results that come from the solution of
differential equations, the main type of result is a solution
trajectory.  In these cases, both the dependent and independent
variables are typically represented as floating point numbers.

But these are not the only types of data a simulation or other
numerical analysis might yield.  From the Modelica world, we might
easily have results that are either integers, booleans or strings
(since these are all fundamental built-in types in Modelica).  But why
not hierarchical data structures (as represented by records in
Modelica) as well?

\subsection{#6 - Metadata}

% 6) It must be possible to associate metadata with entities in the
% file.

One issue with data files is that if you don't provide a means for
associating metadata with entities in the file, the metadata will {\b
become} entities in the file.  For this reason, we deemed it important
that metadata should be treated in a ``first-class'' way.
Specifically, it should be possible to associated metadata with the
file as a whole, with data structures in the file all the way down to
individual signals.  This would allow tools to persist other important
information, beyond the solution, in these data files.  For example,
information about common plots or plotting options, descriptions of
the signals, units or display units could all be managed in a
structured way without being confused with data and without needing to
be a formal part of the file format specification.

\subsection{#7 - Hierarchy}

% 7) It should be possible to organize information hierarchically.

Many tools create structures that are hierarchical.  In the Modelica
world, we have deep hierarchies of instances in simulated models.  We
also have hierarchies for packages and the definitions contained in
them.  So it is important the a file format can represent this
hierarchies in some way.

In our experience, trying to organize results according to an instance
hierarchy creates quite a bit of complexity.  While tools could
exploit some of the previous requirements (primarily #5 and #6) to
achieve a hierarchical representation, we've found that simply
encoding hierarchy in the names of variables (*e.g.*
\code{car.engine.crankshaft.tau}) is typically sufficient and can
avoid considerable complexity.

\subsection{#8 - Easy Translation}

% Support for heterogenous data (no strict type constraints)

Even though our goal is to have a format that is well suited to web
and cloud based application, it should also be capable of representing
the kinds of simulation results as we normally think of them.  For
this reason, we include as a requirement the ability to represent the
widely used ``dsres'' format used by many tools including Dymola and
OpenModelica.  In particular, it should be possible to preserve all
the data normally contained in these formats when translating them
into the \recon formats.  Furthermore, the \recon formats should have
comparable storage efficiency (*i.e.,* the files should be
approximately the same size, regardless of format).

\section{Approach}

\subsection{Reading vs. Writing}

In reviewing these requirements, the main design challenge was trying
to reconcile requirements #1 and #2.  Implementing requirement #1
typically involves the need to write data out one row at a time where
each row represents the values of all the variables for successive
solution times.  As such, the solution values for any particular {\i
  variable} are widely spaced.  However, requirement #2 requires us to
be able to extract a given variable with a minimum number of I/O
operations.  In other words, requirement #1 typically results in data
being ``fragmented'' while requirement #2 depends on that data being
clustered together.

% Two formats
% clocks

Our solution to this design problem was to design two file formats.
The first, the \wall format, is designed for writing.  Not only does
it make adding data fast and efficient, it also supports, unlike the
\code{dsres} approach, adding data for multiple ``tables'' at once.
In practice, what this means is that if you have variables in your
simulation that are partitioned such that they have different
independent variables (as with the new clock semantics in Modelica
3.3), this format supports writing out new data for any of these
variables.  In other words, you can add results that may have
completely different time bases.

The other format, the \meld format, is optimized for reading.
Specifically, it is optimized for requirement #2.  For an ideal
format, it would be possible to extract a single result trajectory in
a single read.  This would act to minimize the impact of latency.
Furthermore, the bytes read should contain only data associated with
the desired signal.  This would minimize the impact of limited
bandwidth.  As we will discuss shortly, the \meld file manages to
achieve these performance characteristics for all but the first signal
read.

Our expectation is that tools will write data out (during simulation)
in the \wall format.  Tools may choose to keep the data in this
format.  For platforms where network access is not a requirement, the
write optimized nature of the \wall format will probably be adequate
for both reading and writing.  However, for cases where data will be
read over a network we expect that tools will, upon completing a
simulation, rewrite their data into the \meld format.

\subsection{Serialization}

% Started with BSON, switched to MsgPack

There are really two aspects each \recon format.  The first is the
structure of the file (where different pieces of information reside in
the file, something we'll discuss in Section \ref{sec:spec}) and the
other is how the actual data is represented.

Obviously, the data is represented as individual bytes.  So we must
define the process by which multi-byte pieces of information (*e.g.,*
floating point numbers) are ``serialized'' into bytes.  One of the
implicit goals of this project was to make a file format that was easy
to read and write.  Since serializing and deserializing data was a big
part of the implementation, we could make the implementation much
easier if we leveraged existing standards for serialization and
deserialization.

One of the interesting things about coming from the web and cloud
based application side is the ubiquity of JSON notation.  While the
Javascript language itself has many ``unusual'' semantics, the syntax
and semantics around serialization and deserialization are
surprisingly simple and intuitive.  Unlike XML, for example, writing a
parser for JSON and then mapping into a native language representation
is surprisingly easy and widely supported.

However, JSON is a textual representation.  The problem with a textual
representation is the additional overhead of having to parse and
interpret the text and convert, without any loss of precision, into a
binary representation.  For this reason, we didn't consider JSON by
itself a practice approach to serialization and deserialization.

While we wanted to avoid the parsing aspect of JSON, the JSON data
model \cite{JSON} is well suited to our purposes and many different
groups have attended to create a binary representation that follows
the JSON data model.  So our initial approach was to consider BSON
\cite{BSON} which is a binary format the is formally specified and
widely implemented because it is one of the cornerstones of the
MongoDB database\cite{MongoDB}.

Unfortunately, the BSON serialization scheme has a significant
drawback.  The way it serializes arrays is very space inefficient.
This is because JSON itself supports sparse indexing of arrays.  As a
result, a serialization must include, for each value in the array, the
index as well.  This adds significant overhead.  There is no way to
specify that all elements in the array or sequential.  As such, there
is no way to avoid this significant penalty.

Fortunately, our reference implementation in Python\cite{pyRecon} had
a clean separation between the serialization scheme and the structural
aspects.  This made it very easy for us to experiment without other
serialization techniques.  We investigated other similar serialization
schemes like Smile\cite{Smile}, BJSON\cite{BJSON} and
UBJSON\cit{UBJSON}.  However, none of them seemed to have a critical
mass behind them.  Indeed, the landscape of binary JSON
representations seemed to be surprisingly fragmented.

As part of our investigation, we also looked at \msgpack.  It turned
out that, like BSON, \msgpack was formally specified and implemented
for a wide variety of platforms\cite{MsgPack}.  Furthermore, in
testing \msgpack, we found that it had much better storage efficiency
compared to BSON.  So, in the end, we moved forward using the \msgpack
serialization scheme.

The \msgpack approach had a couple of unanticipated benefits.  In
\msgpack, floating point numbers can be encoded in either single or
double precision representations.  Also, the specification identifies
and formally specifies several different optimizations to minimize the
number of bytes required to store short integers or short strings.
The underlying ``types'' permitted in this format map very easily into
the JSON format which, in turn, means that it maps well into the
native data types common across all the languages we are interested
in.  Finally, the \msgpack serialization scheme includes an extension
mechanism for including additional data types beyond those in the
specification.  While we don't have any immediate use for these
extensions, it is nice to have that feature if we ever find that
\msgpack's serialization is a constraint.

\section{Specification}
\label{sec:spec}

With the motivation behind us, let's turn to the actual specification
of these formats.  In this section we will describe the layout of both
the \wall format and the \meld format.  As mentioned in the previous
section, the serialization is done using \msgpack.  So we will focus
mainly on the layout of data within the file.  When describing the
actual data being stored we will use JSON notation document the data
with the implicit understanding that this data will be serialized and
deserialized using \msgpack.

% tables and objects


\subsection{Wall Format}
\label{sec:wall_spec}

Recall that the \wall format is optimized for reading and that this,
in turn, means being able to easily add data.  You can think of the
\wall format is being similar to a brick wall where each brick (new
piece of data) is staggered with respect to others.  As you will see,
we are {\b not} storing information in homogenous arrays and this means
that we cannot predict the index of data simply based on information
about which row or column it is in.  Also note that it is possible to
have data from two different tables interleaved between each other.
This allows us to add data with two distinct time bases.  But it makes
the location of data even more difficult to predict.  However,
remember that the \wall format is optimized for writing, not reading
and that if network access is required tools will typically rewrite
their data into the \meld format.

Each \wall file starts with the following sequence of bytes:

\begin{lstlisting}[frame=single]  % Start your code-block
0x72 0x65 0x63 0x6f 0x6e 0x31 0x77 0x61 0x6c 0x6c 0x3a 0x76 0x30 0x31
\end{lstlisting}

This is a hex encoding of the ASCII string \code{recon:wall:v01}.
This allows us to identify whether this is a \recon \wall file and, if
so, what version of the specification should be applied.

The next four bytes are a binary encoding of the length of the header.
This encoding is done in so-called ``network byte order''
(big-endian).

Once the length of the header is known, the bytes for the header are
read in.  These bytes are assumed to have been serialized in \msgpack
format so we must next unpack these bytes.  Once unpacked, the header
should contain the following information:

\lstset{language=javascript}
\begin{lstlisting}[frame=single]  % Start your code-block
{
  "m": {<file-level metadata>},
  "t": [<list of table objects]>,
  "o": [<list of objects>]
}
\end{lstlisting}

The \code{"m"} key is associated with the value for any file level
metadata.  This metadata is, itself, represented as a map in \msgpack
and will be deserialized as such into the implementation language.
The \code{"t"} key is associated with the list of tables present in
the file.  Each of these table objects is serialized as follows:

\lstset{language=javascript}
\begin{lstlisting}[frame=single]  % Start your code-block
{
  "m": {<table-level metadata>},
  "s": [<list of signals contained in the table>],
  "a": {
    <aliasname>: {
      "v": <base signal name>,
      "t": <transform string>
    }
  },
  "v": {
    <varname>: {<variable-level metadata}
  }
}
\end{lstlisting}

Again, the \code{"m"} key is associated with metadata (represented as
a \msgpack map) but this time it is metadata associated with the
table.  In addition, we have the \code{"s"} key which represents an
ordered list of signals.  A signal represents an actual solution
trajectory and the order is important because the order in this list
indicates the order in which the data will be stored in successive
rows (to be discussed shortly).

The \code{"a"} key represents any aliases present in the table.
Again, this is a \msgpack map where the name of the alias is the key.
There are two essential pieces of information associated with each of
the aliases.  The first, stored under the \code{"v"} key is the name
of the base signal 

Finally, we have the \code{"v"} key which is a map where the keys are
the names of variables (*i.e.,* both signals and aliases)

% Summary

\subsection{Meld Format}
\label{sec:meld_spec}

\lstset{language=python}
\begin{lstlisting}[frame=single]  % Start your code-block
f()
\end{lstlisting}

\subsection{Transformations}

% Discuss transformation grammar

\section{Discussion}
\label{sec:discussion}

% Caching

\subsection{Performance}

% Space efficiency

\subsection{Conversion}

% Conversions

\subsection{Use Case}

% # of reads

\subsection{Metadata}

% Metadata connected to everything

\subsection{Transforms}

\section{Conclusion}
\label{sec:conclusion}
%TODO

\bibliography{recon}
\end{document}
